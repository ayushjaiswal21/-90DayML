{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Project - Data Transformation with PySpark\n",
    "\n",
    "This practice project focuses on data transformation and integration using PySpark. You will work with two datasets, perform various transformations such as adding columns, renaming columns, dropping unnecessary columns, joining dataframes, and writing results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages\n",
    "!pip install wget pyspark findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, quarter, to_date, when, lit, sum, avg, col\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkContext object\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Data Transformation Project\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load datasets into PySpark DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets using wget\n",
    "import wget\n",
    "\n",
    "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
    "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
    "\n",
    "print(\"Downloading dataset1.csv...\")\n",
    "wget.download(link_to_data1)\n",
    "print(\"\\nDownloading dataset2.csv...\")\n",
    "wget.download(link_to_data2)\n",
    "print(\"\\nDatasets downloaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into PySpark dataframes\n",
    "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"DataFrames loaded successfully\")\n",
    "print(f\"df1 shape: {df1.count()} rows, {len(df1.columns)} columns\")\n",
    "print(f\"df2 shape: {df2.count()} rows, {len(df2.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Display the schema of both dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Schema of df1:\")\n",
    "df1.printSchema()\n",
    "\n",
    "print(\"\\nSchema of df2:\")\n",
    "df2.printSchema()\n",
    "\n",
    "print(\"\\nSample data from df1:\")\n",
    "df1.show(5)\n",
    "\n",
    "print(\"\\nSample data from df2:\")\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Add a new column to each dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names first\n",
    "print(\"Columns in df1:\", df1.columns)\n",
    "print(\"Columns in df2:\", df2.columns)\n",
    "\n",
    "# Add new column year to df1\n",
    "date_columns_df1 = [col for col in df1.columns if 'date' in col.lower()]\n",
    "if date_columns_df1:\n",
    "    date_col_df1 = date_columns_df1[0]\n",
    "    # Use the correct date format for the data (M/d/yyyy)\n",
    "    df1 = df1.withColumn('year', year(to_date(date_col_df1, 'M/d/yyyy')))\n",
    "    print(f\"Added year column to df1 using {date_col_df1}\")\n",
    "else:\n",
    "    print(\"No date column found in df1\")\n",
    "\n",
    "# Add new column quarter to df2\n",
    "date_columns_df2 = [col for col in df2.columns if 'date' in col.lower()]\n",
    "if date_columns_df2:\n",
    "    date_col_df2 = date_columns_df2[0]\n",
    "    # Use the correct date format for the data (M/d/yyyy)\n",
    "    df2 = df2.withColumn('quarter', quarter(to_date(date_col_df2, 'M/d/yyyy')))\n",
    "    print(f\"Added quarter column to df2 using {date_col_df2}\")\n",
    "else:\n",
    "    print(\"No date column found in df2\")\n",
    "\n",
    "print(\"\\nUpdated df1 schema:\")\n",
    "df1.printSchema()\n",
    "print(\"\\nUpdated df2 schema:\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Rename columns in both dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename df1 column amount to transaction_amount\n",
    "if 'amount' in df1.columns:\n",
    "    df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
    "    print(\"Renamed 'amount' to 'transaction_amount' in df1\")\n",
    "else:\n",
    "    print(\"'amount' column not found in df1\")\n",
    "\n",
    "# Rename df2 column value to transaction_value\n",
    "if 'value' in df2.columns:\n",
    "    df2 = df2.withColumnRenamed('value', 'transaction_value')\n",
    "    print(\"Renamed 'value' to 'transaction_value' in df2\")\n",
    "else:\n",
    "    print(\"'value' column not found in df2\")\n",
    "\n",
    "print(\"\\nUpdated columns in df1:\", df1.columns)\n",
    "print(\"Updated columns in df2:\", df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Drop unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns description and location from df1\n",
    "columns_to_drop_df1 = [col for col in ['description', 'location'] if col in df1.columns]\n",
    "if columns_to_drop_df1:\n",
    "    df1 = df1.drop(*columns_to_drop_df1)\n",
    "    print(f\"Dropped {columns_to_drop_df1} from df1\")\n",
    "else:\n",
    "    print(\"No columns to drop from df1\")\n",
    "\n",
    "# Drop column notes from df2\n",
    "if 'notes' in df2.columns:\n",
    "    df2 = df2.drop('notes')\n",
    "    print(\"Dropped 'notes' from df2\")\n",
    "else:\n",
    "    print(\"'notes' column not found in df2\")\n",
    "\n",
    "print(\"\\nFinal columns in df1:\", df1.columns)\n",
    "print(\"Final columns in df2:\", df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Join dataframes based on a common column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df1 and df2 based on common column customer_id\n",
    "if 'customer_id' in df1.columns and 'customer_id' in df2.columns:\n",
    "    joined_df = df1.join(df2, 'customer_id', 'inner')\n",
    "    print(\"Successfully joined df1 and df2 on customer_id\")\n",
    "    print(\"Joined dataframe columns:\", joined_df.columns)\n",
    "    print(\"Joined dataframe count:\", joined_df.count())\n",
    "    joined_df.show(5)\n",
    "else:\n",
    "    print(\"Warning: customer_id column not found in both dataframes\")\n",
    "    print(\"df1 columns:\", df1.columns)\n",
    "    print(\"df2 columns:\", df2.columns)\n",
    "    joined_df = df1  # Use df1 as fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Filter data based on a condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for transaction amount > 1000\n",
    "if 'transaction_amount' in joined_df.columns:\n",
    "    filtered_df = joined_df.filter(\"transaction_amount > 1000\")\n",
    "    print(\"Filtered dataframe for transaction_amount > 1000\")\n",
    "    print(\"Filtered dataframe count:\", filtered_df.count())\n",
    "    filtered_df.show(5)\n",
    "else:\n",
    "    print(\"Warning: transaction_amount column not found\")\n",
    "    print(\"Available columns:\", joined_df.columns)\n",
    "    filtered_df = joined_df  # Use joined_df as fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Aggregate data by customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer_id and aggregate the sum of transaction amount\n",
    "if 'customer_id' in filtered_df.columns and 'transaction_amount' in filtered_df.columns:\n",
    "    total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
    "    print(\"Total amount per customer:\")\n",
    "    total_amount_per_customer.show()\n",
    "else:\n",
    "    print(\"Warning: Required columns not found for aggregation\")\n",
    "    print(\"Available columns:\", filtered_df.columns)\n",
    "    total_amount_per_customer = filtered_df.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Write the result to a Hive table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write total_amount_per_customer to a Hive table named customer_totals\n",
    "try:\n",
    "    total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")\n",
    "    print(\"Successfully saved data to Hive table: customer_totals\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save to Hive table. Error: {str(e)[:200]}...\")\n",
    "    print(\"This is expected in a local environment without Hive setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Write the filtered data to HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write filtered_df to HDFS in parquet format\n",
    "try:\n",
    "    # Use a local path since HDFS is not available\n",
    "    filtered_df.coalesce(1).write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")\n",
    "    print(\"Successfully saved filtered data to parquet file: filtered_data.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save to parquet. Error: {str(e)[:200]}...\")\n",
    "    # Try saving as CSV as an alternative\n",
    "    try:\n",
    "        filtered_df.coalesce(1).write.mode(\"overwrite\").csv(\"filtered_data_csv\", header=True)\n",
    "        print(\"Successfully saved filtered data as CSV: filtered_data_csv\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Could not save as CSV either: {str(e2)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Add a new column based on a condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column with value indicating whether transaction amount is > 5000 or not\n",
    "if 'transaction_amount' in df1.columns:\n",
    "    df1 = df1.withColumn(\"high_value\", when(col(\"transaction_amount\") > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))\n",
    "    print(\"Added high_value column to df1\")\n",
    "    df1.select(\"transaction_amount\", \"high_value\").show(10)\n",
    "else:\n",
    "    print(\"Warning: transaction_amount column not found in df1\")\n",
    "    print(\"Available columns:\", df1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12: Calculate the average transaction value per quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average transaction value for each quarter in df2\n",
    "if 'quarter' in df2.columns and 'transaction_value' in df2.columns:\n",
    "    average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n",
    "    print(\"Average transaction value per quarter:\")\n",
    "    average_value_per_quarter.show()\n",
    "else:\n",
    "    print(\"Warning: Required columns not found for quarter aggregation\")\n",
    "    print(\"Available columns in df2:\", df2.columns)\n",
    "    average_value_per_quarter = df2.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13: Write the result to a Hive table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write average_value_per_quarter to a Hive table named quarterly_averages\n",
    "try:\n",
    "    average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")\n",
    "    print(\"Successfully saved data to Hive table: quarterly_averages\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save to Hive table. Error: {str(e)[:200]}...\")\n",
    "    print(\"This is expected in a local environment without Hive setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14: Calculate the total transaction value per year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total transaction value for each year in df1\n",
    "if 'year' in df1.columns and 'transaction_amount' in df1.columns:\n",
    "    total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n",
    "    print(\"Total transaction value per year:\")\n",
    "    total_value_per_year.show()\n",
    "else:\n",
    "    print(\"Warning: Required columns not found for year aggregation\")\n",
    "    print(\"Available columns in df1:\", df1.columns)\n",
    "    total_value_per_year = df1.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 15: Write the result to HDFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write total_value_per_year to HDFS in the CSV format\n",
    "try:\n",
    "    total_value_per_year.coalesce(1).write.mode(\"overwrite\").csv(\"total_value_per_year.csv\", header=True)\n",
    "    print(\"Successfully saved data to CSV file: total_value_per_year.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save to CSV. Error: {str(e)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed the lab.\n",
    "\n",
    "This practice project provides hands-on experience with data transformation and integration using PySpark. You've performed various tasks including:\n",
    "- Loading and examining data\n",
    "- Adding and renaming columns\n",
    "- Dropping unnecessary columns\n",
    "- Joining dataframes\n",
    "- Filtering and aggregating data\n",
    "- Writing results to different formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
