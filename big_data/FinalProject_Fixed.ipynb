{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Data Analysis using Spark\n",
    "\n",
    "This final project focuses on data analysis using Spark SQL. You will create a DataFrame by loading data from a CSV file and apply transformations and actions using Spark SQL.\n",
    "\n",
    "**Tasks Overview:**\n",
    "1. Generate DataFrame from CSV data\n",
    "2. Define a schema for the data\n",
    "3. Display schema of DataFrame\n",
    "4. Create a temporary view\n",
    "5. Execute an SQL query\n",
    "6. Calculate Average Salary by Department\n",
    "7. Filter and Display IT Department Employees\n",
    "8. Add 10% Bonus to Salaries\n",
    "9. Find Maximum Salary by Age\n",
    "10. Self-Join on Employee Data\n",
    "11. Calculate Average Employee Age\n",
    "12. Calculate Total Salary by Department\n",
    "13. Sort Data by Age and Salary\n",
    "14. Count Employees in Each Department\n",
    "15. Filter Employees with the letter 'o' in the Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required packages\n",
    "!pip install pyspark findspark wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, max, sum, count\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkContext object\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Final Project - Employee Data Analysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CSV data\n",
    "import wget\n",
    "print(\"Downloading employees.csv...\")\n",
    "wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")\n",
    "print(\"\\nEmployees data downloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Generate a Spark DataFrame from the CSV data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the employees CSV file and import it into a DataFrame variable named \"employees_df\"\n",
    "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "print(\"Successfully loaded employees.csv into DataFrame\")\n",
    "print(f\"Number of rows: {employees_df.count()}\")\n",
    "print(f\"Number of columns: {len(employees_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Define a schema for the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Schema for the input data and read the file using the user-defined Schema\n",
    "print(\"Current DataFrame schema:\")\n",
    "employees_df.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "employees_df.show(5)\n",
    "\n",
    "# Define a schema based on the expected structure\n",
    "schema = StructType([\n",
    "    StructField(\"Emp_No\", IntegerType(), True),\n",
    "    StructField(\"Emp_Name\", StringType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file with the defined schema\n",
    "employees_df = spark.read.csv(\"employees.csv\", header=True, schema=schema)\n",
    "print(\"\\nDataFrame loaded with defined schema:\")\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Display schema of DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns of the DataFrame, along with their respective data types\n",
    "print(\"Schema of employees_df DataFrame:\")\n",
    "employees_df.printSchema()\n",
    "\n",
    "print(\"\\nColumn names and data types:\")\n",
    "for field in employees_df.schema.fields:\n",
    "    print(f\"Column: {field.name}, Data Type: {field.dataType}, Nullable: {field.nullable}\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "employees_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Create a temporary view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view named \"employees\" for the DataFrame\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "print(\"Successfully created temporary view 'employees'\")\n",
    "\n",
    "# Verify the view is created by running a simple query\n",
    "result = spark.sql(\"SELECT COUNT(*) as total_employees FROM employees\")\n",
    "result.show()\n",
    "print(\"Temporary view is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Execute an SQL query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to fetch solely the records from the View where the age exceeds 30\n",
    "query = \"SELECT * FROM employees WHERE Age > 30\"\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "print(\"Employees with age > 30:\")\n",
    "result_df.show()\n",
    "\n",
    "print(f\"\\nTotal number of employees with age > 30: {result_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Calculate Average Salary by Department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to calculate the average salary of employees grouped by department\n",
    "query = \"SELECT Department, AVG(Salary) as Average_Salary FROM employees GROUP BY Department ORDER BY Average_Salary DESC\"\n",
    "avg_salary_by_dept = spark.sql(query)\n",
    "\n",
    "print(\"Average salary by department:\")\n",
    "avg_salary_by_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: Filter and Display IT Department Employees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a filter to select records where the department is 'IT'\n",
    "it_employees = employees_df.filter(employees_df.Department == 'IT')\n",
    "\n",
    "print(\"IT Department employees:\")\n",
    "it_employees.show()\n",
    "\n",
    "print(f\"\\nTotal number of IT employees: {it_employees.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8: Add 10% Bonus to Salaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n",
    "employees_with_bonus = employees_df.withColumn(\"SalaryAfterBonus\", col(\"Salary\") * 1.10)\n",
    "\n",
    "print(\"Employees with salary after 10% bonus:\")\n",
    "employees_with_bonus.select(\"Emp_No\", \"Emp_Name\", \"Salary\", \"SalaryAfterBonus\").show()\n",
    "\n",
    "# Update the main dataframe\n",
    "employees_df = employees_with_bonus\n",
    "print(\"\\nDataFrame updated with bonus column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9: Find Maximum Salary by Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by age and calculate the maximum salary for each age group\n",
    "max_salary_by_age = employees_df.groupBy(\"Age\").agg(max(\"Salary\").alias(\"Max_Salary\")).orderBy(\"Age\")\n",
    "\n",
    "print(\"Maximum salary by age:\")\n",
    "max_salary_by_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10: Self-Join on Employee Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrame with itself based on the \"Emp_No\" column\n",
    "# Create aliases for the dataframes to distinguish columns\n",
    "employees_alias1 = employees_df.alias(\"emp1\")\n",
    "employees_alias2 = employees_df.alias(\"emp2\")\n",
    "\n",
    "# Self-join on Emp_No (this will create a cartesian product of matching Emp_No)\n",
    "self_joined = employees_alias1.join(employees_alias2, \n",
    "                                   employees_alias1.Emp_No == employees_alias2.Emp_No, \n",
    "                                   \"inner\")\n",
    "\n",
    "print(\"Self-joined DataFrame on Emp_No:\")\n",
    "# Select specific columns to make the output more readable\n",
    "self_joined.select(\"emp1.Emp_No\", \"emp1.Emp_Name\", \"emp1.Salary\", \n",
    "                  \"emp2.Emp_No\", \"emp2.Emp_Name\", \"emp2.Salary\").show(10)\n",
    "\n",
    "print(f\"\\nTotal records after self-join: {self_joined.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Calculate Average Employee Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average age of employees\n",
    "avg_age = employees_df.agg(avg(\"Age\").alias(\"Average_Age\"))\n",
    "\n",
    "print(\"Average age of employees:\")\n",
    "avg_age.show()\n",
    "\n",
    "# Alternative: collect the value and print it\n",
    "avg_age_value = avg_age.collect()[0][\"Average_Age\"]\n",
    "print(f\"\\nThe average age of employees is: {avg_age_value:.2f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Calculate Total Salary by Department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total salary for each department using GroupBy and Aggregate functions\n",
    "total_salary_by_dept = employees_df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"Total_Salary\")).orderBy(\"Total_Salary\", ascending=False)\n",
    "\n",
    "print(\"Total salary by department:\")\n",
    "total_salary_by_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 13: Sort Data by Age and Salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by age in ascending order and then by salary in descending order\n",
    "sorted_employees = employees_df.orderBy(\"Age\", col(\"Salary\").desc())\n",
    "\n",
    "print(\"Employees sorted by age (ascending) and salary (descending):\")\n",
    "sorted_employees.select(\"Emp_No\", \"Emp_Name\", \"Age\", \"Salary\", \"Department\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 14: Count Employees in Each Department\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of employees in each department\n",
    "employee_count_by_dept = employees_df.groupBy(\"Department\").agg(count(\"Emp_No\").alias(\"Employee_Count\")).orderBy(\"Employee_Count\", ascending=False)\n",
    "\n",
    "print(\"Number of employees in each department:\")\n",
    "employee_count_by_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 15: Filter Employees with the letter 'o' in the Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a filter to select records where the employee's name contains the letter 'o'\n",
    "employees_with_o = employees_df.filter(col(\"Emp_Name\").contains(\"o\"))\n",
    "\n",
    "print(\"Employees with letter 'o' in their name:\")\n",
    "employees_with_o.select(\"Emp_No\", \"Emp_Name\", \"Salary\", \"Age\", \"Department\").show()\n",
    "\n",
    "print(f\"\\nTotal number of employees with 'o' in their name: {employees_with_o.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You have completed the final project.\n",
    "\n",
    "You have successfully:\n",
    "- ✅ Generated a DataFrame from CSV data\n",
    "- ✅ Defined and applied a schema\n",
    "- ✅ Created temporary views for SQL operations\n",
    "- ✅ Executed various SQL queries and DataFrame operations\n",
    "- ✅ Performed data transformations and aggregations\n",
    "- ✅ Applied filters and sorting operations\n",
    "- ✅ Calculated statistics and insights from the data\n",
    "\n",
    "You now have hands-on experience with Spark SQL and DataFrame operations for data analysis!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
