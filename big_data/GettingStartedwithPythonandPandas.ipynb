{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark  # This helps us find and use Apache Spark\n",
    "findspark.init()  # Initialize findspark to locate Spark\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType\n",
    "import pandas as pd  \n",
    "# Initialize a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"COVID-19 Data Analysis\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check if the Spark Session is active\n",
    "if 'spark' in locals() and isinstance(spark, SparkSession):\n",
    "    print(\"SparkSession is active and ready to use.\")\n",
    "else:\n",
    "    print(\"SparkSession is not active. Please create a SparkSession.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cb184",
   "metadata": {},
   "source": [
    "## 5. Importing data into Pandas from various sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642eaa7",
   "metadata": {},
   "source": [
    "Let's read the COVID-19 data from the provided URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aaa452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the COVID-19 data from the provided URL\n",
    "vaccination_data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KpHDlIzdtR63BdTofl1mOg/owid-covid-latest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f671a11",
   "metadata": {},
   "source": [
    "## 6. Displaying the first five records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e50f5-5e31-4956-b9b7-256e97893868",
   "metadata": {},
   "source": [
    "### To retrieve and print the first five records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7f92e-961c-40f2-9b3c-1a389bf06367",
   "metadata": {},
   "source": [
    "- `vaccination_data.head()` retrieves the first five rows of the DataFrame vaccination_data.This gives us a quick look at the data contained within the data set.\n",
    "- The `print` function is used to display a message indicating what is being shown, followed by the actual data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf06498-3710-4a1d-a947-409cee745bd6",
   "metadata": {},
   "source": [
    "### Selecting specific columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a889c593-465f-4405-9165-bef58bb2d574",
   "metadata": {},
   "source": [
    "- Let\\'s define a list called `columns_to_display`, which contains the names of the columns as : `['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']`.\n",
    "- By using `vaccination_data[columns_to_display].head()`, let\\'s filter the DataFrame to only show the specified columns and again display the first five records of this subset.\n",
    "- The continent column is explicitly converted to string, while the numeric columns (total cases, total deaths, total vaccinations, population) are filled with zeros for NaN values and then converted to int64 (which is compatible with LongType in Spark).\n",
    "- The use of fillna(0) ensures that NaN values do not cause type issues during the Spark DataFrame creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1267201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Displaying the first 5 records of the vaccination data:\")\n",
    "columns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n",
    "# Show the first 5 records\n",
    "print(vaccination_data[columns_to_display].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d9256",
   "metadata": {},
   "source": [
    "## 7. Converting the Pandas DataFrame to a Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c51087",
   "metadata": {},
   "source": [
    "Let\\'s convert the Pandas DataFrame, which contains our COVID-19 vaccination data, into a Spark DataFrame. This conversion is crucial as it allows us to utilize Spark\\'s distributed computing capabilities, enabling us to handle larger datasets and perform operations in a more efficient manner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c305eb-4aff-43f5-91c9-e827f2e6a1c4",
   "metadata": {},
   "source": [
    "### Defining the schema:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8c3ac-9e8a-40fa-a82c-d5eaf1008f8b",
   "metadata": {},
   "source": [
    "- **StructType**: \n",
    "  - A class that defines a structure for a DataFrame.\n",
    "\n",
    "- **StructField**: \n",
    "  - Represents a single field in the schema.\n",
    "  - **Parameters**:\n",
    "    1. **Field name**: The name of the field.\n",
    "    2. **Data type**: The type of data for the field.\n",
    "    3. **Nullable**: A boolean indicating whether null values are allowed.\n",
    "\n",
    "- **Data types**:\n",
    "  - **StringType()**: Used for text fields.\n",
    "  - **LongType()**: Used for numerical fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e48806-3bfb-49ff-8dbf-ce2f4389ac67",
   "metadata": {},
   "source": [
    "### Data type conversion:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e42fb-c167-4933-a081-34681fd9dbc7",
   "metadata": {},
   "source": [
    "- **astype(str)**: \n",
    "  - Used to convert the `'continent'` column to string type.\n",
    "\n",
    "- **fillna(0)**: \n",
    "  - Replaces any NaN values with 0, ensuring that the numerical fields do not contain any missing data.\n",
    "\n",
    "- **astype('int64')**: \n",
    "  - Converts the columns from potentially mixed types to 64-bit integers for consistent numerical representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d348633-eb97-4287-8f74-999969660654",
   "metadata": {},
   "source": [
    "### Creating a Spark DataFrame:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99815da4-edf6-4494-8d4e-290ad1419246",
   "metadata": {},
   "source": [
    "- **createDataFrame**:\n",
    "  - The `createDataFrame` method of the Spark session (`spark`) is called with `vaccination_data` (the Pandas DataFrame) as its argument.\n",
    "  - **Parameters**:\n",
    "    - It takes as input a subset of the pandas DataFrame that corresponds to the fields defined in the schema, accessed using `schema.fieldNames()`.\n",
    "- This function automatically converts the Pandas DataFrame into a Spark DataFrame, which is designed to handle larger data sets across a distributed environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7999fb9-d772-439e-b3c4-bf3d1cc65553",
   "metadata": {},
   "source": [
    "- The resulting spark_df will have the defined schema, which ensures consistency and compatibility with Spark's data processing capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411dfe1-382b-4edb-9d82-c8175e4c3df6",
   "metadata": {},
   "source": [
    "### Storing the result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdde4c4-13b3-43b3-a351-2b082d1d9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame directly\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"total_cases\", LongType(), True),\n",
    "    StructField(\"total_deaths\", LongType(), True),\n",
    "    StructField(\"total_vaccinations\", LongType(), True),\n",
    "    StructField(\"population\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Convert the columns to the appropriate data types\n",
    "vaccination_data['continent'] = vaccination_data['continent'].astype(str)  # Ensures continent is a string\n",
    "vaccination_data['total_cases'] = vaccination_data['total_cases'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "vaccination_data['total_deaths'] = vaccination_data['total_deaths'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "vaccination_data['total_vaccinations'] = vaccination_data['total_vaccinations'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "vaccination_data['population'] = vaccination_data['population'].fillna(0).astype('int64')  # Fill NaNs and convert to int\n",
    "\n",
    "spark_df = spark.createDataFrame(vaccination_data[schema.fieldNames()])  # Use only the specified fields\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efcae4",
   "metadata": {},
   "source": [
    "## 8. Checking the structure of the Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e429aa1",
   "metadata": {},
   "source": [
    "In this section, Let\\'s examine the structure of the Spark DataFrame that we created from the Pandas DataFrame. Understanding the schema of a DataFrame is crucial as it provides insight into the data types of each column and helps ensure that the data is organized correctly for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49166db1-b330-4f52-b99e-910170287f3a",
   "metadata": {},
   "source": [
    "### Displaying the schema:\n",
    "\n",
    "- The method `spark_df.printSchema()` is called to output the structure of the Spark DataFrame.\n",
    "- This method prints the names of the columns along with their data types (e.g., `StringType`, `IntegerType`, `DoubleType`, etc.), providing a clear view of how the data is organized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555080e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Schema of the Spark DataFrame:\")\n",
    "spark_df.printSchema()\n",
    "# Print the structure of the DataFrame (columns and types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3d525",
   "metadata": {},
   "source": [
    "## 9. Basic data exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358f341",
   "metadata": {},
   "source": [
    "In this section, let\\'s perform basic data exploration on the Spark DataFrame. This step is essential for understanding the data set better, allowing us to gain insights and identify any patterns or anomalies. Let\\'s demonstrate how to view specific contents of the DataFrame, select certain columns, and filter records based on conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9f700-26fb-423f-b434-b5a14b849c0a",
   "metadata": {},
   "source": [
    "### 9.1 Viewing DataFrame contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df4892-8b53-4be1-95e6-7eb41a5e5ded",
   "metadata": {},
   "source": [
    "- To view the contents in the DataFrame, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c016e-8623-47f7-8562-7c15254e1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the names of the columns you want to display\n",
    "columns_to_display = ['continent', 'total_cases', 'total_deaths', 'total_vaccinations', 'population']\n",
    "# Display the first 5 records of the specified columns\n",
    "spark_df.select(columns_to_display).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb630c-52a0-4448-bb49-6d217195a169",
   "metadata": {},
   "source": [
    "### 9.2 Picking specific columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1c756-291d-40f0-994f-1b79307c5088",
   "metadata": {},
   "source": [
    "- To display only certain columns, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd6146-c8a8-4761-8b46-8d10e48bb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Displaying the 'continent' and 'total_cases' columns:\")\n",
    "# Show only the 'continent' and 'total_cases' columns\n",
    "spark_df.select('continent', 'total_cases').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d070b-2152-4381-8e46-4060f8c5c0d0",
   "metadata": {},
   "source": [
    "### 9.3 Sifting Through Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85acf63c-4ef6-4b2b-bfa9-e6d479e215e5",
   "metadata": {},
   "source": [
    "- To filter records based on a specific condition, use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c3317-aaf2-4690-8f95-aa5bf6568c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtering records where 'total_cases' is greater than 1,000,000:\")\n",
    " # Show records with more than 1 million total cases\n",
    "spark_df.filter(spark_df['total_cases'] > 1000000).show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb2722",
   "metadata": {},
   "source": [
    "## 10. Working with columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746713d",
   "metadata": {},
   "source": [
    "In this section, let\\'s create a new column called `death_percentage`, which calculates the death rate during the COVID-19 pandemic. This calculation is based on the total_deaths (the count of deaths) and the population (the total population) columns in our Spark DataFrame. This new metric will provide valuable insight into the impact of COVID-19 in different regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76e0c1-10f8-4486-8ec4-2d3e94e9ae67",
   "metadata": {},
   "source": [
    "- Let\\'s import the functions module from `pyspark.sql` as `F`, which contains built-in functions for DataFrame operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b67498-7868-4ea0-abcf-503ee37e3fdc",
   "metadata": {},
   "source": [
    "### Calculating the death percentage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4781768-63d5-450c-ac4e-683a577e5426",
   "metadata": {},
   "source": [
    "- Let\\'s create a new DataFrame `spark_df_with_percentage` by using the `withColumn()` method to add a new column called `death_percentage`.\n",
    "- The formula `(spark_df['total_deaths'] / spark_df['population']) * 100` computes the death percentage by dividing the total deaths by the total population and multiplying by 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52934a8-ac14-414b-acf3-1a8eeb81e2c5",
   "metadata": {},
   "source": [
    "### Formatting the percentage:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37040078-0153-4610-be51-7cd0017f9ca3",
   "metadata": {},
   "source": [
    "- Let\\'s update the `death_percentage` column to format its values to two decimal places using `F.format_number()`, and concatenate a percentage symbol using `F.concat()` and `F.lit('%')`.\n",
    "- This makes the death percentage easier to read and interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d557b00b-0315-4511-becf-6762bb1a713b",
   "metadata": {},
   "source": [
    "### Selecting relevant columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a38bfe-d663-48a8-b544-c0f9b9f8391c",
   "metadata": {},
   "source": [
    "- Let\\'s define a list `columns_to_display` that includes `'total_deaths', 'population', 'death_percentage', 'continent', 'total_vaccinations', and 'total_cases'`.\n",
    "- Finally, let's display the first five records of the modified DataFrame with the new column by calling `spark_df_with_percentage.select(columns_to_display).show(5)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4cd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark_df_with_percentage = spark_df.withColumn(\n",
    "    'death_percentage', \n",
    "    (spark_df['total_deaths'] / spark_df['population']) * 100\n",
    ")\n",
    "spark_df_with_percentage = spark_df_with_percentage.withColumn(\n",
    "    'death_percentage',\n",
    "    F.concat(\n",
    "        # Format to 2 decimal places\n",
    "        F.format_number(spark_df_with_percentage['death_percentage'], 2), \n",
    "        # Append the percentage symbol \n",
    "        F.lit('%')  \n",
    "    )\n",
    ")\n",
    "columns_to_display = ['total_deaths', 'population', 'death_percentage', 'continent', 'total_vaccinations', 'total_cases']\n",
    "spark_df_with_percentage.select(columns_to_display).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317b1d6",
   "metadata": {},
   "source": [
    "## 11. Grouping and summarizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba2c4d",
   "metadata": {},
   "source": [
    " Let\\'s calculate the total number of deaths per continent using the data in our Spark DataFrame. Grouping and summarizing data is a crucial aspect of data analysis, as it allows us to aggregate information and identify trends across different categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562debc-6a53-4b2e-b9b5-126a01a976d7",
   "metadata": {},
   "source": [
    " ### Grouping the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88621b79-42e1-48d6-8d16-6207cc418bdb",
   "metadata": {},
   "source": [
    "The `spark_df.groupby(['continent'])` method groups the data by the `continent` column. This means that all records associated with each continent will be aggregated together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e588b9-8907-4b4e-9d72-5cee84bc1631",
   "metadata": {},
   "source": [
    "### Aggregating the deaths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82e1dc-169b-4e2c-ae90-f8faa52118de",
   "metadata": {},
   "source": [
    "The `agg({\"total_deaths\": \"SUM\"})` function is used to specify the aggregation operation. In this case, we want to calculate the sum of the `total_deaths` for each continent. This operation will create a new DataFrame where each continent is listed alongside the total number of deaths attributed to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92335c9-47d2-4996-8cb5-602ebb50a8bb",
   "metadata": {},
   "source": [
    "### Displaying the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726aec32-ef77-459b-ad91-96f1a02363bf",
   "metadata": {},
   "source": [
    "The `show()` method is called to display the results of the aggregation. This will output the total number of deaths for each continent in a tabular format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed954475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating the total deaths per continent:\")\n",
    "# Group by continent and sum total death rates\n",
    "spark_df.groupby(['continent']).agg({\"total_deaths\": \"SUM\"}).show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee06742",
   "metadata": {},
   "source": [
    "## 12. Exploring user-defined functions (UDFs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32e2025",
   "metadata": {},
   "source": [
    "\n",
    "UDFs in PySpark allow us to create custom functions that can be applied to individual columns within a DataFrame. This feature provides increased flexibility and customization in data processing, enabling us to define specific transformations or calculations that are not available through built-in functions. In this section, let\\'s define a UDF to convert total deaths in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22048a5-07fd-4ff8-bf67-9c28bd2fe44b",
   "metadata": {},
   "source": [
    "### Importing pandas_udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac5f44-dcd5-40a1-832c-f16adcb4b7d9",
   "metadata": {},
   "source": [
    "The `pandas_udf` function is imported from `pyspark.sql.functions`. This decorator allows us to define a UDF that operates on Pandas Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a1b22-6c06-450d-a30f-7042a1ef3bc0",
   "metadata": {},
   "source": [
    "### Defining the UDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7bfc2-cc4f-4f4c-85fc-66d4207f6165",
   "metadata": {},
   "source": [
    "This function `convert_total_deaths()` takes in a parameter `total_deaths` and returns double its value. You can replace the logic with any transformation you want to apply to the column data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42d792-fb46-4c1f-b54a-86da693ea0fd",
   "metadata": {},
   "source": [
    "### Registering the UDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21035ca1-61a8-4d1e-bc2d-f9aa0ab2788f",
   "metadata": {},
   "source": [
    "The line `spark.udf.register(\"convert_total_deaths\", convert_total_deaths, IntegerType())` registers the UDF with Spark indicating that the function returns an integer, allowing us to use it in Spark SQL queries and DataFrame operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724fa2ec-6a45-4f26-9555-040e129b3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Function definition\n",
    "def convert_total_deaths(total_deaths):\n",
    "    return total_deaths * 2\n",
    "# Here you can define any transformation you want\n",
    "# Register the UDF with Spark\n",
    "spark.udf.register(\"convert_total_deaths\", convert_total_deaths, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40388f73",
   "metadata": {},
   "source": [
    "## 13. Using Spark SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3148b",
   "metadata": {},
   "source": [
    "Spark SQL enables us to execute SQL queries directly on DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86424dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the existing temporary view if it exists\n",
    "spark.sql(\"DROP VIEW IF EXISTS data_v\")\n",
    "\n",
    "# Create a new temporary view\n",
    "spark_df.createTempView('data_v')\n",
    "\n",
    "# Execute the SQL query using the UDF\n",
    "spark.sql('SELECT continent, total_deaths, convert_total_deaths(total_deaths) as converted_total_deaths FROM data_v').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0860017d",
   "metadata": {},
   "source": [
    "## 14. Running SQL queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9a201",
   "metadata": {},
   "source": [
    "In this step, let\\'s execute SQL queries to retrieve specific records from the temporary view which was created earlier. Let\\'s demonstrate how to display all records from the data table and filter those records based on vaccination totals. This capability allows for efficient data exploration and analysis using SQL syntax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff2ce1-2b36-4056-8d13-47b13ece5864",
   "metadata": {},
   "source": [
    "## Displaying All Records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f37109-ab25-4fc8-b8c3-f4d57992ae7b",
   "metadata": {},
   "source": [
    "The first query retrieves all records from the temporary view data using the SQL command SELECT * FROM data_v. The show() method is called to display the results in a tabular format. This is useful for getting an overview of the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd78c13-a5c8-4d06-a0bb-bfb85225db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM data_v').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804612f2-1d75-413d-b0fd-26ad05d1a9da",
   "metadata": {},
   "source": [
    "### Filtering records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd1fb9-e27f-4612-be6b-883791bb4187",
   "metadata": {},
   "source": [
    "The second query is designed to filter the data set to show only those continents where the total vaccinations exceed 1 million. The SQL command used here is `SELECT continent FROM data_v WHERE total_vaccinations > 1000000`. The `show()` method is again used to display the results, specifically listing the continents that meet the filter criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146173a-0b9f-477b-b81c-552309c52e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Displaying continent with total vaccinated more than 1 million:\")\n",
    "# SQL filtering\n",
    "spark.sql(\"SELECT continent FROM data_v WHERE total_vaccinations > 1000000\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "45a0dd703bfbcc37ede89eec12aa2a3ff03ef47840aa569cd67704c66ddb0f6c"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
