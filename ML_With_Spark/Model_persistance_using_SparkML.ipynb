{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4c36a3-1114-4fda-b85c-6ca0942c3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#import functions/Classes for sparkml\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# import functions/Classes for metrics\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e4352-0ddd-4bcc-98d6-b5eef2df8673",
   "metadata": {},
   "source": [
    "# Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc9ee2-ebb8-464e-b0d6-28f051817a44",
   "metadata": {},
   "source": [
    "## Task 1 - Create a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a38e20-b89f-472a-a49d-ec9d36d5cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SparkSession\n",
    "#Ignore any warnings by SparkSession command\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Model Persistence\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be159fe-5558-4fa9-af99-1b2d18c8b8e0",
   "metadata": {},
   "source": [
    "Load the dataset into the spark dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6436f-5595-4532-9cd5-c9cca55bd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the spark.read.csv function we load the data into a dataframe.\n",
    "# the header = True mentions that there is a header row in out csv file\n",
    "# the inferSchema = True, tells spark to automatically find out the data types of the columns.\n",
    "\n",
    "# Load mpg dataset\n",
    "mpg_data = spark.read.csv(\"mpg.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c97bed-7bbf-4dfb-9c6d-609a73bdcbd3",
   "metadata": {},
   "source": [
    "Print the schema of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f8336-b420-4be4-b45e-e6ba9ac0089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ff55b-6eb9-4ca0-ba63-d1c38c3c4e12",
   "metadata": {},
   "source": [
    "Show top 5 rows from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ace47-4e17-4bf7-b664-0f83b3358c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bc03a-45dd-4dc1-b625-71cd6fafaa65",
   "metadata": {},
   "source": [
    "We ask the VectorAssembler to group a bunch of inputCols as single column named \"features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7d332-5d23-48be-800d-4f217c33556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature vector\n",
    "assembler = VectorAssembler(inputCols=[\"Cylinders\", \"Engine Disp\", \"Horsepower\", \"Weight\", \"Accelerate\", \"Year\"], outputCol=\"features\")\n",
    "mpg_transformed_data = assembler.transform(mpg_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e766fcd-8080-4e6c-b61f-c9fe917852c1",
   "metadata": {},
   "source": [
    "Display the assembled \"features\" and the label column \"MPG\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bcccd-89d2-4b64-b0c7-ee8a79ff15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_transformed_data.select(\"features\",\"MPG\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802b7b4-5686-4a88-a58d-1da8d634c47b",
   "metadata": {},
   "source": [
    "We split the data set in the ratio of 70:30. 70% training data, 30% testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085ad5e-747c-4e91-b39a-685caa8b6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "(training_data, testing_data) = mpg_transformed_data.randomSplit([0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb8171-c7f3-4819-8f25-03617197065d",
   "metadata": {},
   "source": [
    "Create a LR model and train the model using the pipeline on training data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ad231-9baf-44f2-824a-a781cb132db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "# Ignore any warnings\n",
    "lr = LinearRegression(labelCol=\"MPG\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9942624-7762-4820-9edb-7c9b17cde130",
   "metadata": {},
   "source": [
    "## Task 2 - Save the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a37e5-f362-4e69-9788-a1aefa817964",
   "metadata": {},
   "source": [
    "Create a folder where the model will to be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c3aeb-5356-491c-8d4c-16f37b019313",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0b3a9-c82b-4a6c-818b-79160a37fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the model to the path \"./model_stoarage/\"\n",
    "\n",
    "model.write().overwrite().save(\"./model_storage/\")\n",
    "\n",
    "#The overwrite method is used to overwrite the model if it already exists,\n",
    "#and the save method is used to specify the path where the model should be saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f2f6f-08e9-4f6a-b00a-fd3aed5dff36",
   "metadata": {},
   "source": [
    "## Task 3 - Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f5b9d-e7ea-47bf-8d77-2d8a599951d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "# Load persisted model\n",
    "loaded_model = PipelineModel.load(\"./model_storage/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6242b7f-23e3-4319-b789-a53247bf73c7",
   "metadata": {},
   "source": [
    "## Task 4 - Predict using the loaded model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d1035-5158-4fcb-94cf-fd4d4e99739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = loaded_model.transform(testing_data)\n",
    "#In the above example, we use the load method of the PipelineModel object to load the persisted model from disk. We can then use this loaded model to make predictions on new data using the transform method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634ded8-45d7-47d7-a3be-0f02ac7fae0b",
   "metadata": {},
   "source": [
    "Your model is now trained. We use the testing data to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76525e-8558-44d3-a77d-53766fffcff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on testing data\n",
    "predictions = model.transform(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3b9cd-37c8-465a-a73b-0a2814b71678",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a18889-3577-40b8-ba22-1271baccbcc2",
   "metadata": {},
   "source": [
    "Stop Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99653cfa-8f67-49bc-abb7-66348d5e9fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2eba6f-fa02-4442-b2d2-53cc15f7cb59",
   "metadata": {},
   "source": [
    "<!--\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-05-04|0.1|Ramesh Sannareddy|Initial Version Created|\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "prev_pub_hash": "fca7b41601010b049ee7b19eaac61d0bb4c1b9d73554f146ab480e08cbd5333a"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
